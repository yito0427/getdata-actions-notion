#!/usr/bin/env python3
"""
å…¨102å–å¼•æ‰€ã®ä¸¦åˆ—èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé«˜é€Ÿç‰ˆï¼‰
å„å–å¼•æ‰€ã®APIã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€ä¸¦åˆ—å®Ÿè¡Œã§é«˜é€ŸåŒ–
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
import sys
import time

sys.path.insert(0, str(Path(__file__).parent.parent))

from explore_all_exchanges import ExchangeExplorer, save_to_notion
from loguru import logger
import ccxt.async_support as ccxt


class ParallelExchangeExplorer(ExchangeExplorer):
    """ä¸¦åˆ—å®Ÿè¡Œç‰ˆã®å–å¼•æ‰€èª¿æŸ»ã‚¯ãƒ©ã‚¹"""
    
    async def explore_all_exchanges_parallel(self, max_concurrent: int = 20):
        """å…¨å–å¼•æ‰€ã‚’ä¸¦åˆ—ã§èª¿æŸ»ï¼ˆæœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼‰"""
        logger.info(f"ğŸš€ {len(self.exchanges)}å–å¼•æ‰€ã®ä¸¦åˆ—èª¿æŸ»ã‚’é–‹å§‹ï¼ˆæœ€å¤§åŒæ™‚å®Ÿè¡Œ: {max_concurrent}ï¼‰")
        
        start_time = time.time()
        
        # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def explore_with_semaphore(exchange_name):
            async with semaphore:
                logger.info(f"ğŸ” {exchange_name} èª¿æŸ»é–‹å§‹")
                result = await self.explore_exchange(exchange_name)
                
                # çµæœã‚’ãƒ­ã‚°
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                data_types = [k for k, v in result.get("available_data", {}).items() if v]
                logger.info(f"{status_icon} {exchange_name}: {len(data_types)}ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½")
                
                return exchange_name, result
        
        # å…¨å–å¼•æ‰€ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        tasks = [explore_with_semaphore(exchange) for exchange in self.exchanges]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’æ ¼ç´
        for item in results:
            if isinstance(item, Exception):
                logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {item}")
            else:
                exchange_name, result = item
                self.results[exchange_name] = result
        
        elapsed_time = time.time() - start_time
        logger.success(f"âœ… èª¿æŸ»å®Œäº†ï¼å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’")
        
        return self.results


async def save_to_notion_batch(explorer: ExchangeExplorer):
    """Notionã¸ã®ãƒãƒƒãƒä¿å­˜ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    if not Config.NOTION_API_KEY or not Config.NOTION_DATABASE_ID:
        logger.error("Notionèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    from src.notion.realdata_uploader import RealDataNotionUploader
    from src.config import Config
    
    uploader = RealDataNotionUploader()
    
    # ã‚µãƒãƒªãƒ¼ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
    summary = explorer.generate_summary()
    
    logger.info(f"ğŸ“Š Notionã¸ã®ä¿å­˜ã‚’é–‹å§‹: {len(explorer.results)}å–å¼•æ‰€")
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’æœ€åˆã«ä½œæˆ
    await save_summary_report(uploader, explorer, summary)
    
    # å„å–å¼•æ‰€ã®è©³ç´°ã‚’ä¸¦åˆ—ã§ä¿å­˜ï¼ˆNotion APIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
    sorted_exchanges = sorted(
        explorer.results.items(),
        key=lambda x: sum(x[1].get("available_data", {}).values()),
        reverse=True
    )
    
    # Notion APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦ãƒãƒƒãƒå‡¦ç†
    batch_size = 10  # 10ä»¶ãšã¤å‡¦ç†
    for i in range(0, len(sorted_exchanges), batch_size):
        batch = sorted_exchanges[i:i+batch_size]
        
        logger.info(f"ğŸ“¤ Notionã¸ãƒãƒƒãƒä¿å­˜ä¸­... [{i+1}-{min(i+batch_size, len(sorted_exchanges))}/{len(sorted_exchanges)}]")
        
        # ãƒãƒƒãƒå†…ã¯é †æ¬¡å‡¦ç†ï¼ˆNotion APIåˆ¶é™ã®ãŸã‚ï¼‰
        for exchange_name, data in batch:
            await save_exchange_detail_fast(uploader, exchange_name, data)
            await asyncio.sleep(0.3)  # Notion APIåˆ¶é™å¯¾ç­–
        
        # ãƒãƒƒãƒé–“ã§å°‘ã—å¾…æ©Ÿ
        if i + batch_size < len(sorted_exchanges):
            await asyncio.sleep(1)
    
    logger.success(f"âœ… å…¨{len(sorted_exchanges)}å–å¼•æ‰€ã®ãƒ‡ãƒ¼ã‚¿ã‚’Notionã«ä¿å­˜å®Œäº†ï¼")


async def save_summary_report(uploader, explorer, summary):
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
    client = uploader.client
    
    title = f"ğŸ“Š 102å–å¼•æ‰€ä¸¦åˆ—èª¿æŸ»çµæœ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    
    properties = {
        "Name": {"title": [{"text": {"content": title}}]},
        "Data Type": {"select": {"name": "Exchange Survey"}},
        "Collection Time": {"date": {"start": datetime.now().isoformat()}},
        "Total Tickers": {"number": summary["data_availability"]["ticker"]},
        "Total OrderBooks": {"number": summary["data_availability"]["orderbook"]},
        "Record Count": {"number": summary["successful"]},
        "Status": {"select": {"name": "Completed"}}
    }
    
    children = [
        {
            "object": "block",
            "type": "heading_1",
            "heading_1": {"rich_text": [{"text": {"content": "ğŸŒ 102å–å¼•æ‰€ãƒ‡ãƒ¼ã‚¿èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆï¼ˆä¸¦åˆ—å®Ÿè¡Œç‰ˆï¼‰"}}]}
        },
        {
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{
                    "text": {
                        "content": f"âœ… èª¿æŸ»å®Œäº†: {summary['total_exchanges']}å–å¼•æ‰€\n"
                                  f"âœ… æˆåŠŸ: {summary['successful']}å–å¼•æ‰€\n"
                                  f"âŒ å¤±æ•—: {summary['failed']}å–å¼•æ‰€"
                    }
                }],
                "icon": {"emoji": "ğŸš€"}
            }
        }
    ]
    
    await client.pages.create(
        parent={"database_id": Config.NOTION_DATABASE_ID},
        properties=properties,
        children=children
    )


async def save_exchange_detail_fast(uploader, exchange_name: str, data: dict):
    """å–å¼•æ‰€è©³ç´°ã‚’ä¿å­˜ï¼ˆãƒ‡ãƒ¼ã‚¿è©³ç´°å«ã‚€ï¼‰"""
    from src.config import Config
    client = uploader.client
    
    data_types = [k for k, v in data.get("available_data", {}).items() if v]
    title = f"ğŸ¢ {exchange_name} | {data.get('total_markets', 0)} markets | {len(data_types)} types"
    
    properties = {
        "Name": {"title": [{"text": {"content": title}}]},
        "Data Type": {"select": {"name": "Exchange Analysis"}},
        "Exchange": {"select": {"name": exchange_name}},
        "Collection Time": {"date": {"start": datetime.now().isoformat()}},
        "Total Tickers": {"number": data.get("total_markets", 0)},
        "Record Count": {"number": len(data_types)},
        "Status": {"select": {"name": "Success" if data["status"] == "success" else "Failed"}}
    }
    
    # è©³ç´°ãªãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æ§‹ç¯‰
    children = [
        {
            "object": "block",
            "type": "heading_2",
            "heading_2": {"rich_text": [{"text": {"content": f"ğŸ“Š {exchange_name} ãƒ‡ãƒ¼ã‚¿è©³ç´°"}}]}
        }
    ]
    
    # åŸºæœ¬æƒ…å ±
    basic_info = f"âœ… å…¬é–‹API: {'åˆ©ç”¨å¯èƒ½' if data.get('has_public_api') else 'åˆ©ç”¨ä¸å¯'}\n"
    basic_info += f"ğŸ“Š ç·ãƒãƒ¼ã‚±ãƒƒãƒˆæ•°: {data.get('total_markets', 0)}\n"
    basic_info += f"ğŸ”§ å–å¾—å¯èƒ½ãƒ‡ãƒ¼ã‚¿ç¨®é¡: {len(data_types)}ç¨®é¡"
    
    children.append({
        "object": "block",
        "type": "paragraph",
        "paragraph": {"rich_text": [{"text": {"content": basic_info}}]}
    })
    
    # ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã®è©³ç´°
    if data.get("available_data"):
        data_details = "ğŸ“‹ **å–å¾—å¯èƒ½ãƒ‡ãƒ¼ã‚¿:**\n"
        
        # Ticker
        if data["available_data"].get("ticker"):
            data_details += "âœ… Ticker (ä¾¡æ ¼æƒ…å ±)\n"
            if data.get("ticker_sample"):
                s = data["ticker_sample"]
                data_details += f"  â€¢ æœ€çµ‚ä¾¡æ ¼: ${s.get('last')}\n"
                data_details += f"  â€¢ Bid/Ask: ${s.get('bid')} / ${s.get('ask')}\n"
                data_details += f"  â€¢ å–å¼•é‡: {s.get('volume')}\n"
        else:
            data_details += "âŒ Ticker\n"
        
        # OrderBook
        if data["available_data"].get("orderbook"):
            data_details += "âœ… OrderBook (æ¿æƒ…å ±)\n"
            if data.get("orderbook_sample"):
                ob = data["orderbook_sample"]
                data_details += f"  â€¢ è²·ã„/å£²ã‚Šæ³¨æ–‡: {ob.get('bids')}/{ob.get('asks')}ä»¶\n"
                data_details += f"  â€¢ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰: {ob.get('spread')}\n"
        else:
            data_details += "âŒ OrderBook\n"
        
        # Trades
        if data["available_data"].get("trades"):
            data_details += f"âœ… Trades: {data.get('trades_count', 0)}ä»¶\n"
        else:
            data_details += "âŒ Trades\n"
        
        # OHLCV
        if data["available_data"].get("ohlcv"):
            data_details += "âœ… OHLCV (ãƒ­ãƒ¼ã‚½ã‚¯è¶³)\n"
            if data.get("ohlcv_timeframes"):
                data_details += f"  â€¢ æ™‚é–“è»¸: {', '.join(data['ohlcv_timeframes'][:5])}\n"
        else:
            data_details += "âŒ OHLCV\n"
        
        children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": data_details}}]}
        })
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒ³ãƒœãƒ«
    if data.get("sample_symbols"):
        children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{
                    "text": {"content": f"ğŸ’± å–æ‰±é€šè²¨ãƒšã‚¢ä¾‹: {', '.join(data['sample_symbols'][:10])}"}
                }]
            }
        })
    
    # APIæ©Ÿèƒ½
    if data.get("api_features"):
        api_info = "ğŸ”§ **APIæ©Ÿèƒ½:**\n"
        for feature, available in data["api_features"].items():
            if feature != "timeframes":
                api_info += f"{'âœ…' if available else 'âŒ'} {feature}\n"
        
        children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": api_info}}]}
        })
    
    # JSONãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
    children.append({
        "object": "block",
        "type": "code",
        "code": {
            "rich_text": [{
                "text": {"content": json.dumps(data, ensure_ascii=False, indent=2)[:1000]}
            }],
            "language": "json",
            "caption": [{"text": {"content": "èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ï¼ˆæŠœç²‹ï¼‰"}}]
        }
    })
    
    await client.pages.create(
        parent={"database_id": Config.NOTION_DATABASE_ID},
        properties=properties,
        children=children
    )


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ å…¨102å–å¼•æ‰€ã®ä¸¦åˆ—èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("âš¡ ä¸¦åˆ—å®Ÿè¡Œã«ã‚ˆã‚Šå‡¦ç†æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã—ã¾ã™")
    
    # èª¿æŸ»å®Ÿè¡Œ
    explorer = ParallelExchangeExplorer()
    
    # å…¨102å–å¼•æ‰€ã‚’ä¸¦åˆ—èª¿æŸ»ï¼ˆæœ€å¤§20åŒæ™‚å®Ÿè¡Œï¼‰
    results = await explorer.explore_all_exchanges_parallel(max_concurrent=20)
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    with open(output_dir / "exchange_survey_parallel.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary = explorer.generate_summary()
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼")
    logger.info("="*60)
    logger.info(f"èª¿æŸ»å–å¼•æ‰€æ•°: {summary['total_exchanges']}")
    logger.info(f"æˆåŠŸ: {summary['successful']}")
    logger.info(f"å¤±æ•—: {summary['failed']}")
    logger.info("")
    logger.info("ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½æ€§:")
    for data_type, count in summary['data_availability'].items():
        logger.info(f"  {data_type}: {count}å–å¼•æ‰€")
    
    # Notionã«ä¿å­˜
    logger.info("\nğŸ“¤ Notionã¸ã®ä¿å­˜ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from src.config import Config
    
    await save_to_notion_batch(explorer)
    
    logger.success("\nâœ… å…¨102å–å¼•æ‰€ã®ä¸¦åˆ—èª¿æŸ»ã¨Notionä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    logger.info("ğŸ‘‰ Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    asyncio.run(main())